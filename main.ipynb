{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Classification in Intravascular Ultrasound (3 Separate CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version : 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import graphviz\n",
    "import seaborn as sns\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense, concatenate, MaxPooling2D, Conv2D, Conv2DTranspose, \\\n",
    "UpSampling2D, BatchNormalization, Reshape, Flatten, Dropout\n",
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "import os\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(\"Tensorflow Version : \"+tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels1 = \"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\train\\Train_label_cn1.npy\"\n",
    "trainLabels2 = \"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\train\\Train_label_cn2.npy\"\n",
    "trainLabels3 = \"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\train\\Train_label_cn3.npy\"\n",
    "trainData    = \"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\train\\TrainData.npy\"\n",
    "\n",
    "\n",
    "# Number of Classes\n",
    "n_Classes = 2\n",
    "\n",
    "y_Noise1 = np.load(trainLabels1)\n",
    "y_Noise2 = np.load(trainLabels2)\n",
    "y_Noise3 = np.load(trainLabels3)\n",
    "X_train  = np.load(trainData)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "\n",
    "X_train1, X_val1, y_train_Noise1, y_val_Noise1 = train_test_split(X_train, y_Noise1, test_size=0.20, random_state=42)\n",
    "X_train2, X_val2, y_train_Noise2, y_val_Noise2 = train_test_split(X_train, y_Noise2, test_size=0.20, random_state=42)\n",
    "X_train3, X_val3, y_train_Noise3, y_val_Noise3 = train_test_split(X_train, y_Noise3, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train_Noise1 = np_utils.to_categorical(y_train_Noise1, n_Classes)\n",
    "y_val_Noise1 = np_utils.to_categorical(y_val_Noise1, n_Classes)\n",
    "\n",
    "y_train_Noise2 = np_utils.to_categorical(y_train_Noise2, n_Classes)\n",
    "y_val_Noise2 = np_utils.to_categorical(y_val_Noise2, n_Classes)\n",
    "\n",
    "y_train_Noise3 = np_utils.to_categorical(y_train_Noise3, n_Classes)\n",
    "y_val_Noise3 = np_utils.to_categorical(y_val_Noise3, n_Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optional step.\n",
    "# Noise 1 : Shadows\n",
    "# Noise 2 : Bifurcation\n",
    "# Noise 3 : Branching\n",
    "# Classes = ['shadow', 'bifurcation', 'branching']\n",
    "y = np.stack((y_Noise1,y_Noise2,y_Noise3), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing Data\n",
    "#### This step is necessary if and only if the neural network is deep and has many layers and variables to train.\n",
    "#### Note: Please do not run this cell if its not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Shape (384, 384, 1) Shrinking to (224, 224, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "# Resizing Images\n",
    "\n",
    "image_num = X_train.shape[0]\n",
    "image_cols = 224\n",
    "image_rows = 224\n",
    "image_channels = 1\n",
    "\n",
    "def resizeData(npArray, image_num, image_rows, image_cols, image_channels):\n",
    "    print(\"Found Shape \"+str(npArray[0].shape)+\" Shrinking to (\"+str(image_rows)+\", \"+str(image_cols)+\", \"\\\n",
    "          +str(image_channels)+\")\")\n",
    "    tempArray = np.ndarray((image_num, image_rows, image_cols, image_channels), dtype=np.uint16)\n",
    "    for n in range(npArray.shape[0]):\n",
    "        tempArray[n] = resize(npArray[n], (image_rows, image_cols, image_channels), preserve_range=True)\n",
    "    return tempArray\n",
    "        \n",
    "    \n",
    "X_train = resizeData(X_train, image_num, image_rows, image_cols, image_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printReport(y_test, y_preds):\n",
    "    print(classification_report(y_test, y_preds)) \n",
    "    cm = confusion_matrix(y_test, y_preds)\n",
    "    sns.set(font_scale=1.5) \n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax = sns.heatmap(cm, annot=True, linewidths=1.5, square=True, linecolor=\"Blue\", \n",
    "                    cmap=\"Blues\", \n",
    "                    fmt=\"d\", annot_kws={\"size\": 15})\n",
    "    ax.set(xlabel='Predicted label', ylabel='True label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Operations\n",
    "# 1. Rotation range 180\n",
    "# 2. Horizontal and Vertical Flip\n",
    "\n",
    "trainDataGen1 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "trainDataGen2 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "trainDataGen3 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "valDataGen1 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "valDataGen2 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "valDataGen3 = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "trainDataGen1.fit(X_train1, augment=True, seed=1)\n",
    "trainDataGen2.fit(X_train2, augment=True, seed=1)\n",
    "trainDataGen3.fit(X_train3, augment=True, seed=1)\n",
    "valDataGen1.fit(X_val1, augment=True, seed=1)\n",
    "valDataGen2.fit(X_val2, augment=True, seed=1)\n",
    "valDataGen3.fit(X_val3, augment=True, seed=1)\n",
    "\n",
    "trainDataFlow1 = trainDataGen1.flow(X_train1, y_train_Noise1, batch_size=1, shuffle=True, seed=1)\n",
    "trainDataFlow2 = trainDataGen2.flow(X_train2, y_train_Noise2, batch_size=1, shuffle=True, seed=1)\n",
    "trainDataFlow3 = trainDataGen3.flow(X_train3, y_train_Noise3, batch_size=1, shuffle=True, seed=1)\n",
    "valDataFlow1 = valDataGen1.flow(X_val1, y_val_Noise1, batch_size=1, shuffle=True, seed=1)\n",
    "valDataFlow2 = valDataGen1.flow(X_val2, y_val_Noise2, batch_size=1, shuffle=True, seed=1)\n",
    "valDataFlow3 = valDataGen1.flow(X_val3, y_val_Noise3, batch_size=1, shuffle=True, seed=1)\n",
    "# TrainDataGenerator.fit(X_train, augment=True, seed=1)\n",
    "# trainDataGenerator = TrainDataGenerator.flow(X_train, y_Noise3, batch_size=1, shuffle=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A test to display all augmented data\n",
    "# for x, y in TrainDataGenerator.flow(X_train, y_Noise1, batch_size=1, shuffle=True, seed=1):\n",
    "#     print(\"------------------------------------------------------\")\n",
    "#     print(\"Label : {0}\".format(y))\n",
    "#     plt.subplot(1,1,1)\n",
    "#     plt.imshow(x[0,:,:,0], cmap=plt.get_cmap('gray'))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.show()\n",
    "#     print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 384, 384, 1)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 384, 384, 64)      1664      \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 192, 192, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 192, 192, 128)     204928    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 96, 96, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 96, 96, 256)       819456    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 48, 48, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 48, 48, 512)       3277312   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 24, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 24, 24, 512)       6554112   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "full_conn1 (Dense)           (None, 100)               7372900   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 18,230,574\n",
      "Trainable params: 18,230,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define Model\n",
    "img_height = X_train.shape[1]\n",
    "img_width = X_train.shape[2]\n",
    "img_channels = X_train.shape[3]\n",
    "\n",
    "inputs = Input((img_height, img_width, img_channels))\n",
    "\n",
    "# Block 1\n",
    "xC_1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='block1_conv1', activation='relu')(inputs)\n",
    "# xC_1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='block1_conv2', activation='relu')(xC_1)\n",
    "xP_1 = MaxPooling2D((2,2), name='block1_pool')(xC_1)\n",
    "# f1 = x\n",
    "\n",
    "# Block 2\n",
    "xC_2 = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='block2_conv1', activation='relu')(xP_1)\n",
    "# xC_2 = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='block2_conv2', activation='relu')(xC_2)\n",
    "xP_2 = MaxPooling2D((2,2), name='block2_pool')(xC_2)\n",
    "# f2 = x\n",
    "\n",
    "# Block 3\n",
    "xC_3 = Conv2D(256, (5, 5), strides=(1, 1), padding='same', name='block3_conv1', activation='relu')(xP_2)\n",
    "# xC_3 = Conv2D(256, (5, 5), strides=(1, 1), padding='same', name='block3_conv2', activation='relu')(xC_3)\n",
    "# xC_3 = Conv2D(256, (5, 5), strides=(1, 1), padding='same', name='block3_conv3', activation='relu')(xC_3)\n",
    "xP_3 = MaxPooling2D((2,2), name='block3_pool')(xC_3)\n",
    "# f3 = x\n",
    "\n",
    "# Block 4\n",
    "xC_4 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block4_conv1', activation='relu')(xP_3)\n",
    "# xC_4 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block4_conv2', activation='relu')(xC_4)\n",
    "# xC_4 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block4_conv3', activation='relu')(xC_4)\n",
    "xP_4 = MaxPooling2D((2,2), name='block4_pool')(xC_4)\n",
    "# f4 = x\n",
    "\n",
    "# Block 5\n",
    "xC_5 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block5_conv1', activation='relu')(xP_4)\n",
    "# xC_5 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block5_conv2', activation='relu')(xC_5)\n",
    "# xC_5 = Conv2D(512, (5, 5), strides=(1, 1), padding='same', name='block5_conv3', activation='relu')(xC_5)\n",
    "xP_5 = MaxPooling2D((2,2), name='block5_pool')(xC_5)\n",
    "# f5 = x\n",
    "\n",
    "flat = Flatten(name='flatten')(xP_5)\n",
    "dense = Dense(100, activation='relu', name='full_conn1')(flat)\n",
    "output = Dense(2 , activation='sigmoid', name='predictions')(dense)\n",
    "\n",
    "vgg_model_n1 = Model(inputs, output)\n",
    "vgg_model_n2 = Model(inputs, output)\n",
    "vgg_model_n3 = Model(inputs, output)\n",
    "# vgg_model.load_weights(PATH)\n",
    "vgg_model_n1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "optimizer = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, verbose=0, mode='auto', cooldown=0,  min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "X_test = np.load(\"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\test\\TestData.npy\")\n",
    "y_test1 = np.load(\"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\test\\Test_label_cn1.npy\")\n",
    "y_test2 = np.load(\"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\test\\Test_label_cn2.npy\")\n",
    "y_test3 = np.load(\"D:\\Chirag B\\ComputerVision\\IVUSNoiseClassification\\IVUSNoiseClassification\\data\\\\test\\Test_label_cn3.npy\")\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model For Noise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 22s 248ms/step - loss: 1.5265 - acc: 0.9023 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.18594, saving model to models/model_n1_2019-01-27.hdf5\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.18594\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.18594\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.18594\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.18594\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.18594\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.18594\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.18594\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.18594\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.18594\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.18594\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.18594\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.18594\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.18594\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.18594\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.18594\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.18594\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.18594\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.18594\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.18594\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.18594\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.18594\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.18594\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.18594\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.18594\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.18594\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.18594\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.18594\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.18594\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.18594\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.18594\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.18594\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.18594\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.18594\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.18594\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.18594\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.18594\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.18594\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.18594\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.18594\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.18594\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.18594\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.18594\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.18594\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.18594\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.18594\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.18594\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.18594\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.18594\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.18594\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.18594\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.18594\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.18594\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.18594\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.18594\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.18594\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.18594\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.18594\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.18594\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.18594\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.18594\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.18594\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.18594\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.18594\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.18594\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.18594\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.18594\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.18594\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.18594\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.18594\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.18594\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.18594\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.18594\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.18594\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.18594\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.18594\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.18594\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.18594\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.18594\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.18594\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.18594\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.18594\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.18594\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.18594\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.18594\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.18594\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.18594\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.18594\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.18594\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.18594\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.18594\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.18594\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.18594\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.18594\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.18594\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.18594\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.18594\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.18594\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.18594\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 1.4740 - acc: 0.9080 - val_loss: 2.1859 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.18594\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_1 = ModelCheckpoint('models/model_n1_2019-01-27.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "vgg_model_n1.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# history = vgg_model.fit(X_train, y, validation_split=0.2, batch_size=2, epochs=EPOCHS, callbacks=[model_checkpoint])\n",
    "\n",
    "history_n1 = vgg_model_n1.fit_generator(trainDataFlow1,\n",
    "                    steps_per_epoch=len(X_train1),\n",
    "                    validation_data=valDataFlow1,\n",
    "                    validation_steps=len(X_val1),\n",
    "                    epochs=EPOCHS,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[model_checkpoint_1, reduce_lr],\n",
    "                    class_weight='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate On Test Set and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = vgg_model_n1.predict(X_test, batch_size=9, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       268\n",
      "           1       0.00      0.00      0.00        58\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       326\n",
      "   macro avg       0.41      0.50      0.45       326\n",
      "weighted avg       0.68      0.82      0.74       326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printReport(y_test1, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model For Noise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.86113, saving model to models/model_n2_2019-01-27.hdf5\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 8s 86ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.86113\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.86113\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 7s 86ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 5.86113\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.86113\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 5.86113\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.86113\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 8s 86ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.86113\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.86113\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.86113\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.86113\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.86113\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.86113\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.86113\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.86113\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.86113\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.86113\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.86113\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.86113\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.86113\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 5.86113\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5.86113\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 5.86113\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 5.86113\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 5.86113\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 8s 88ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 5.86113\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 5.86113\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 5.86113\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 5.86113\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 8s 88ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 5.86113\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 5.86113\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 8s 88ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 5.86113\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 8s 88ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 5.86113\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 8s 88ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 5.86113\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 5.86113\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 5.86113\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 5.86113\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 5.86113\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 5.86113\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 5.86113\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 5.86113\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 5.86113\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 5.86113\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 5.86113\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 5.86113\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 5.86113\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 5.86113\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 5.86113\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 5.86113\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 5.86113\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 5.86113\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 5.86113\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 8s 93ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 5.86113\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 5.86113\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 5.86113\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 5.86113\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 5.86113\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 5.86113\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 5.86113\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 5.86113\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 5.86113\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 5.86113\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 5.86113\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 5.86113\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 5.86113\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 8s 92ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 5.86113\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 5.86113\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 5.86113\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 5.86113\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 5.86113\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 5.86113\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 8s 91ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 5.86113\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 5.86113\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 5.86113\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 5.86113\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 5.86113\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 5.86113\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 5.86113\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 5.86113\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 5.86113\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 5.86113\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 5.86113\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 5.86113\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 5.86113\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 5.86113\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 5.86113\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 8s 90ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 5.86113\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 5.86113\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 79ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 5.86113\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 5.86113\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 5.86113\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 5.86113\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 5.86113\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 5.86113\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 8s 89ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 5.86113\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 7s 86ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 5.86113\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 5.86113\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 5.86113\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 5.86113\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 7.9664 - acc: 0.5057 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 5.86113\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_2 = ModelCheckpoint('models/model_n2_2019-01-27.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "vgg_model_n2.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# history = vgg_model.fit(X_train, y, validation_split=0.2, batch_size=2, epochs=EPOCHS, callbacks=[model_checkpoint])\n",
    "\n",
    "history_n2 = vgg_model_n2.fit_generator(trainDataFlow2,\n",
    "                    steps_per_epoch=len(X_train2),\n",
    "                    validation_data=valDataFlow2,\n",
    "                    validation_steps=len(X_val2),\n",
    "                    epochs=EPOCHS,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[model_checkpoint_2, reduce_lr],\n",
    "                    class_weight='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate On Test Set and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      1.00      0.61       144\n",
      "           1       0.00      0.00      0.00       182\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       326\n",
      "   macro avg       0.22      0.50      0.31       326\n",
      "weighted avg       0.20      0.44      0.27       326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_2 = vgg_model_n2.predict(X_test, batch_size=9, verbose=1)\n",
    "printReport(y_test2, preds_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model For Noise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.86113, saving model to models/model_n3_2019-01-27.hdf5\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.86113\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.86113\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 5.86113\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.86113\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 5.86113\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.86113\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.86113\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.86113\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.86113\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.86113\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.86113\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.86113\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.86113\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.86113\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.86113\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.86113\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.86113\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.86113\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.86113\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 5.86113\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5.86113\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 5.86113\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 5.86113\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 5.86113\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 5.86113\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 5.86113\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 5.86113\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 5.86113\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 5.86113\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 5.86113\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 5.86113\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 5.86113\n",
      "Epoch 34/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 5.86113\n",
      "Epoch 35/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 5.86113\n",
      "Epoch 36/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 5.86113\n",
      "Epoch 37/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 5.86113\n",
      "Epoch 38/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 5.86113\n",
      "Epoch 39/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 5.86113\n",
      "Epoch 40/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 5.86113\n",
      "Epoch 41/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 5.86113\n",
      "Epoch 42/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 5.86113\n",
      "Epoch 43/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 5.86113\n",
      "Epoch 44/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 5.86113\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 5.86113\n",
      "Epoch 46/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 5.86113\n",
      "Epoch 47/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 5.86113\n",
      "Epoch 48/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 5.86113\n",
      "Epoch 49/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 5.86113\n",
      "Epoch 50/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 5.86113\n",
      "Epoch 51/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 5.86113\n",
      "Epoch 52/100\n",
      "87/87 [==============================] - 8s 86ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 5.86113\n",
      "Epoch 53/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 5.86113\n",
      "Epoch 54/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 5.86113\n",
      "Epoch 55/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 5.86113\n",
      "Epoch 56/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 5.86113\n",
      "Epoch 57/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 5.86113\n",
      "Epoch 58/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 5.86113\n",
      "Epoch 59/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 5.86113\n",
      "Epoch 60/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 5.86113\n",
      "Epoch 61/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 5.86113\n",
      "Epoch 62/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 5.86113\n",
      "Epoch 63/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 5.86113\n",
      "Epoch 64/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 5.86113\n",
      "Epoch 65/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 5.86113\n",
      "Epoch 66/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 5.86113\n",
      "Epoch 67/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 5.86113\n",
      "Epoch 68/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 5.86113\n",
      "Epoch 69/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 5.86113\n",
      "Epoch 70/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 5.86113\n",
      "Epoch 71/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 5.86113\n",
      "Epoch 72/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 5.86113\n",
      "Epoch 73/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 5.86113\n",
      "Epoch 74/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 5.86113\n",
      "Epoch 75/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 5.86113\n",
      "Epoch 76/100\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 5.86113\n",
      "Epoch 77/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 5.86113\n",
      "Epoch 78/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 5.86113\n",
      "Epoch 79/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 5.86113\n",
      "Epoch 80/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 5.86113\n",
      "Epoch 81/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 5.86113\n",
      "Epoch 82/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 5.86113\n",
      "Epoch 83/100\n",
      "87/87 [==============================] - 7s 86ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 5.86113\n",
      "Epoch 84/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 5.86113\n",
      "Epoch 85/100\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 5.86113\n",
      "Epoch 86/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 5.86113\n",
      "Epoch 87/100\n",
      "87/87 [==============================] - 7s 84ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 5.86113\n",
      "Epoch 88/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 5.86113\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 5.86113\n",
      "Epoch 90/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 5.86113\n",
      "Epoch 91/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 5.86113\n",
      "Epoch 92/100\n",
      "87/87 [==============================] - 7s 85ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 5.86113\n",
      "Epoch 93/100\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 5.86113\n",
      "Epoch 94/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 5.86113\n",
      "Epoch 95/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 5.86113\n",
      "Epoch 96/100\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 5.86113\n",
      "Epoch 97/100\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 5.86113\n",
      "Epoch 98/100\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 5.86113\n",
      "Epoch 99/100\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 5.86113\n",
      "Epoch 100/100\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 6.2990 - acc: 0.6092 - val_loss: 5.8611 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 5.86113\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_3 = ModelCheckpoint('models/model_n3_2019-01-27.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "vgg_model_n3.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# history = vgg_model.fit(X_train, y, validation_split=0.2, batch_size=2, epochs=EPOCHS, callbacks=[model_checkpoint])\n",
    "\n",
    "history_n3 = vgg_model_n3.fit_generator(trainDataFlow3,\n",
    "                    steps_per_epoch=len(X_train3),\n",
    "                    validation_data=valDataFlow3,\n",
    "                    validation_steps=len(X_val3),\n",
    "                    epochs=EPOCHS,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[model_checkpoint_3, reduce_lr],\n",
    "                    class_weight='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate On Test Set and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      1.00      0.54       120\n",
      "           1       0.00      0.00      0.00       206\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       326\n",
      "   macro avg       0.18      0.50      0.27       326\n",
      "weighted avg       0.14      0.37      0.20       326\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chira\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_3 = vgg_model_n3.predict(X_test, batch_size=9, verbose=1)\n",
    "printReport(y_test3, preds_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
